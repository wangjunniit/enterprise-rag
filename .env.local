# ===================== 数据库配置 =====================
# PostgreSQL数据库连接配置
PG_HOST=localhost          # 数据库主机地址，本地部署使用localhost，远程部署填写实际IP
PG_PORT=5432              # 数据库端口，PostgreSQL默认端口为5432
PG_USER=postgres          # 数据库用户名
PG_PASSWORD=123456      # 数据库密码，生产环境请使用强密码
PG_DB=rag                 # 数据库名称，用于存储文档向量和元数据

# ===================== 模型配置 =====================
# 使用Qwen3系列模型，确保模型名称正确
EMBEDDING_MODEL=Qwen/Qwen3-Embedding-0.6B    # 文档向量化模型，将文本转换为768维向量
RERANK_MODEL=Qwen/Qwen3-Reranker-0.6B        # 检索结果重排序模型，提升检索精度
LLM_MODEL=Qwen/Qwen3-0.6B                    # 大语言模型，用于生成最终答案

# ===================== Hugging Face 镜像配置 =====================
# 使用国内镜像站加速模型下载
HF_ENDPOINT=https://hf-mirror.com             # Hugging Face 镜像站地址

# ===================== 模型运行配置 =====================
# 模型运行环境配置
DEVICE=auto               # 设备选择：auto(自动选择), cpu(强制CPU), cuda(强制GPU)
MODEL_CACHE_DIR=./models  # 模型缓存目录，首次运行会下载模型到此目录
MAX_MEMORY_GB=8          # 最大内存使用量(GB)，根据服务器配置调整

# ===================== 文档处理配置 =====================
# 文档分块和检索参数，影响检索效果和性能
CHUNK_SIZE=400           # 文档分块大小(token数)，较大的块包含更多上下文但可能降低精度
CHUNK_OVERLAP=100        # 分块重叠部分(token数)，避免重要信息在分块边界丢失
TOP_K=10                 # 向量检索召回的文档块数量，越大召回越全面但计算量越大
TOP_N=5                  # 重排序后选取的文档块数量，注入到LLM的上下文数量
HISTORY_ROUNDS=5         # 多轮对话保留的历史轮数，影响对话连贯性
MAX_HISTORY_TOKENS=800   # 历史对话最大token数，防止上下文过长

# ===================== 应用配置 =====================
# Web应用服务配置
APP_HOST=0.0.0.0         # 应用监听地址，0.0.0.0表示监听所有网卡，127.0.0.1仅本地访问
APP_PORT=8000            # 应用监听端口，确保端口未被占用
DEBUG=false              # 调试模式，生产环境建议设为false，开发时可设为true
LOG_LEVEL=INFO           # 日志级别：DEBUG, INFO, WARNING, ERROR

# ===================== 安全配置 =====================
# 安全和性能限制配置
MAX_FILE_SIZE_MB=100     # 单个文件最大大小限制(MB)，防止过大文件影响性能
MAX_BATCH_SIZE=50        # 批量处理最大文件数，防止一次处理过多文件导致内存溢出

# ===================== 模型推理配置 =====================
# 模型输入输出长度限制
EMBEDDING_MAX_LENGTH=512    # 向量化模型最大输入长度
RERANK_MAX_LENGTH=512       # 重排序模型最大输入长度
LLM_INPUT_MAX_LENGTH=1024   # LLM输入最大长度
LLM_OUTPUT_MAX_LENGTH=2048  # LLM输出最大长度
LLM_TEMPERATURE=0.7         # LLM生成温度，控制输出的随机性

# ===================== API接口配置 =====================
# API接口参数配置
QUESTION_MAX_LENGTH=1000    # 用户问题最大长度
DEFAULT_PAGE_SIZE=100       # 默认分页大小
SEARCH_DEFAULT_LIMIT=20     # 搜索默认限制
CHUNKS_PAGE_SIZE=50         # 文档分块分页大小
BATCH_COMMIT_SIZE=10        # 批量提交大小

# ===================== 系统配置 =====================
# 系统运行参数
LOG_FILE_NAME=rag_app.log           # 日志文件名
TOKEN_ESTIMATE_RATIO=2.0            # Token估算比例(字符/token)
CHUNKS_PER_FILE_ESTIMATE=10         # 每个文件预估分块数
CONTENT_PREVIEW_LENGTH=200          # 内容预览长度
SEARCH_CONTENT_PREVIEW_LENGTH=300   # 搜索结果内容预览长度